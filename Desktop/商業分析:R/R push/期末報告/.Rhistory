Real2 = test$Chance.of.Admit
Predict2 = predicted_RF
RF_matrix <- table(Real2, Predict2)
sum(diag(RF_matrix))/sum(RF_matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(RF_matrix), mapping = aes(x = Real2, y = Predict2)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#logit
library(arm)
logit <- bayesglm(Chance.of.Admit ~ ., data = train, family = binomial(link='logit'))
step(logit)
logit <- bayesglm(Chance.of.Admit ~ TOEFL.Score + SOP + CGPA, data = train, family = binomial(link='logit'))
predicted_LG <- predict(logit, newdata = test, type = "response")
library(InformationValue)
optcutoff <- optimalCutoff(test$Chance.of.Admit, predicted_LG)
optcutoff
predicted_LG <- ifelse(predicted_LG > optcutoff, 1, 0)
Real3 = test$Chance.of.Admit
Predict3 = predicted_LG
LG_matrix <- table(Real3, Predict3)
sum(diag(LG_matrix))/sum(LG_matrix)
plotROC(test$Chance.of.Admit, Predict3)
sensitivity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
specificity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
#視覺???混淆矩???
ggplot(data =  as.data.frame(LG_matrix), mapping = aes(x = Real3, y = Predict3)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#KNN
library(class)
range <- 1:round(0.2 * nrow(train))
accuracies <- rep(NA, length(range))
for (i in range) {
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = i)
KNN_matrix <- table(test$Chance.of.Admit, predicted_KNN)
accuracies[i] <- sum(diag(KNN_matrix))/sum(KNN_matrix)
}
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = which.max(accuracies))
plot(range, accuracies, xlab = "k")
which.max(accuracies) #k值
Real4 = test$Chance.of.Admit
Predict4 = predicted_KNN
KNN_matrix <- table(Real4, Predict4)
sum(diag(KNN_matrix))/sum(KNN_matrix) #???測???確???= 0.95
Predict4 = predicted_KNN
KNN_matrix <- table(Real4, Predict4)
sum(diag(KNN_matrix))/sum(KNN_matrix) #???測???確???= 0.95
#視覺???混淆矩???
ggplot(data =  as.data.frame(KNN_matrix), mapping = aes(x = Real4, y = Predict4)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.92") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_matrix), mapping = aes(x = Real5, y = Predict5)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.98") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
after_tune <- svm(Chance.of.Admit ~ ., data=train, kernel="radial", cost=1, gamma=0.5)
summary(after_tune)
pred <- predict(after_tune,test)
Real6 = test$Chance.of.Admit
Predict6 = pred
SVM_Tuned_Matrix <- table(Real6, Predict6)
sum(diag(SVM_Tuned_Matrix))/sum(SVM_Tuned_Matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_Tuned_Matrix), mapping = aes(x = Real6, y = Predict6)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
plot(svm_tune)
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
# SVM
library(e1071)
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
pred <- predict(after_tune,test)
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_matrix), mapping = aes(x = Real5, y = Predict5)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.98") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
ADM <- read.csv("Admission_Predict_Ver1.1.csv")
#???ƳB?z
library(dummies)
ADM$University.Rating <- factor(ADM$University.Rating,
levels = c(unique(ADM$University.Rating)))
ADM <- dummy.data.frame(ADM)
ADM$University.Rating1 = NULL
#?ഫ???ƲŦX???]
ADM$Chance.of.Admit <- ifelse(ADM$Chance.of.Admit >= 0.9, 1, 0)
#?]?߰V?m???M???ն?
library(tidyverse)
set.seed(200)
train <- ADM %>%
group_by(Chance.of.Admit) %>%
sample_frac(0.8)
test <- anti_join(ADM, train, by = 'Serial.No.')
train$Serial.No. = NULL
test$Serial.No. = NULL
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
ADM <- read.csv("Admission_Predict_Ver1.1.csv")
#???ƳB?z
library(dummies)
ADM$University.Rating <- factor(ADM$University.Rating,
levels = c(unique(ADM$University.Rating)))
ADM <- dummy.data.frame(ADM)
ADM$University.Rating1 = NULL
#?ഫ???ƲŦX???]
ADM$Chance.of.Admit <- ifelse(ADM$Chance.of.Admit >= 0.9, 1, 0)
#?]?߰V?m???M???ն?
library(tidyverse)
set.seed(200)
train <- ADM %>%
group_by(Chance.of.Admit) %>%
sample_frac(0.8)
test <- anti_join(ADM, train, by = 'Serial.No.')
train$Serial.No. = NULL
test$Serial.No. = NULL
#決???樹
library(rpart)
tree <- rpart(Chance.of.Admit ~ . , data = train, method = "class")
predicted_DT <- predict(tree, newdata = test, type = "class")
Real1 = test$Chance.of.Admit
Predict1 = predicted_DT
DT_matrix <- table(Real1, Predict1) #confusion table
sum(diag(DT_matrix))/sum(DT_matrix) #???測???確???=0.94
#視覺???混淆矩???
library(ggplot2)
ggplot(data = as.data.frame(DT_matrix), mapping = aes(x = Real1, y = Predict1)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.94") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#視覺???決策樹
library(rpart.plot)
rpart.plot(tree)
#???機森???
library(randomForest)
train$Chance.of.Admit <- as.character(train$Chance.of.Admit)
train$Chance.of.Admit <- as.factor(train$Chance.of.Admit)
rf <- randomForest(Chance.of.Admit ~ ., data = train, importance = TRUE)
#???大概???要幾棵樹
plot(rf)
legend("topright", colnames(rf$err.rate), col = 1:4, cex = 0.8, fill = 1:4)
#??????數?????????
importance(rf)
varImpPlot(rf)
#RF???測
predicted_RF <- predict(rf, newdata = test, type = "class")
Real2 = test$Chance.of.Admit
Predict2 = predicted_RF
RF_matrix <- table(Real2, Predict2)
sum(diag(RF_matrix))/sum(RF_matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(RF_matrix), mapping = aes(x = Real2, y = Predict2)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#logit
library(arm)
logit <- bayesglm(Chance.of.Admit ~ ., data = train, family = binomial(link='logit'))
step(logit)
logit <- bayesglm(Chance.of.Admit ~ TOEFL.Score + SOP + CGPA, data = train, family = binomial(link='logit'))
predicted_LG <- predict(logit, newdata = test, type = "response")
library(InformationValue)
optcutoff <- optimalCutoff(test$Chance.of.Admit, predicted_LG)
optcutoff
predicted_LG <- ifelse(predicted_LG > optcutoff, 1, 0)
Real3 = test$Chance.of.Admit
Predict3 = predicted_LG
LG_matrix <- table(Real3, Predict3)
sum(diag(LG_matrix))/sum(LG_matrix)
plotROC(test$Chance.of.Admit, Predict3)
sensitivity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
specificity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
#視覺???混淆矩???
ggplot(data =  as.data.frame(LG_matrix), mapping = aes(x = Real3, y = Predict3)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#KNN
library(class)
range <- 1:round(0.2 * nrow(train))
accuracies <- rep(NA, length(range))
for (i in range) {
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = i)
KNN_matrix <- table(test$Chance.of.Admit, predicted_KNN)
accuracies[i] <- sum(diag(KNN_matrix))/sum(KNN_matrix)
}
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = which.max(accuracies))
plot(range, accuracies, xlab = "k")
which.max(accuracies) #k值
Real4 = test$Chance.of.Admit
Predict4 = predicted_KNN
KNN_matrix <- table(Real4, Predict4)
sum(diag(KNN_matrix))/sum(KNN_matrix) #???測???確???= 0.92
#視覺???混淆矩???
ggplot(data =  as.data.frame(KNN_matrix), mapping = aes(x = Real4, y = Predict4)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.92") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_matrix), mapping = aes(x = Real5, y = Predict5)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.98") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ .,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
after_tune <- svm(Chance.of.Admit ~ ., data=train, kernel="radial", cost=1, gamma=0.5)
summary(after_tune)
pred <- predict(after_tune,test)
Real6 = test$Chance.of.Admit
Predict6 = pred
SVM_Tuned_Matrix <- table(Real6, Predict6)
sum(diag(SVM_Tuned_Matrix))/sum(SVM_Tuned_Matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_Tuned_Matrix), mapping = aes(x = Real6, y = Predict6)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
ADM <- read.csv("Admission_Predict_Ver1.1.csv")
#???ƳB?z
library(dummies)
ADM$University.Rating <- factor(ADM$University.Rating,
levels = c(unique(ADM$University.Rating)))
ADM <- dummy.data.frame(ADM)
ADM$University.Rating1 = NULL
#?ഫ???ƲŦX???]
ADM$Chance.of.Admit <- ifelse(ADM$Chance.of.Admit >= 0.9, 1, 0)
#?]?߰V?m???M???ն?
library(tidyverse)
set.seed(200)
train <- ADM %>%
group_by(Chance.of.Admit) %>%
sample_frac(0.8)
test <- anti_join(ADM, train, by = 'Serial.No.')
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_matrix), mapping = aes(x = Real5, y = Predict5)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.98") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
after_tune <- svm(Chance.of.Admit ~ ., data=train, kernel="radial", cost=1, gamma=0.5)
ADM <- read.csv("Admission_Predict_Ver1.1.csv")
#???ƳB?z
library(dummies)
ADM$University.Rating <- factor(ADM$University.Rating,
levels = c(unique(ADM$University.Rating)))
ADM <- dummy.data.frame(ADM)
ADM$University.Rating1 = NULL
#?ഫ???ƲŦX???]
ADM$Chance.of.Admit <- ifelse(ADM$Chance.of.Admit >= 0.9, 1, 0)
#?]?߰V?m???M???ն?
library(tidyverse)
set.seed(200)
train <- ADM %>%
group_by(Chance.of.Admit) %>%
sample_frac(0.8)
test <- anti_join(ADM, train, by = 'Serial.No.')
train$Serial.No. = NULL
test$Serial.No. = NULL
#決???樹
library(rpart)
tree <- rpart(Chance.of.Admit ~ . , data = train, method = "class")
predicted_DT <- predict(tree, newdata = test, type = "class")
Real1 = test$Chance.of.Admit
Predict1 = predicted_DT
DT_matrix <- table(Real1, Predict1) #confusion table
sum(diag(DT_matrix))/sum(DT_matrix) #???測???確???=0.94
#視覺???混淆矩???
library(ggplot2)
ggplot(data = as.data.frame(DT_matrix), mapping = aes(x = Real1, y = Predict1)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.94") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#視覺???決策樹
library(rpart.plot)
rpart.plot(tree)
#???機森???
library(randomForest)
train$Chance.of.Admit <- as.character(train$Chance.of.Admit)
train$Chance.of.Admit <- as.factor(train$Chance.of.Admit)
rf <- randomForest(Chance.of.Admit ~ ., data = train, importance = TRUE)
#???大概???要幾棵樹
plot(rf)
legend("topright", colnames(rf$err.rate), col = 1:4, cex = 0.8, fill = 1:4)
#??????數?????????
importance(rf)
varImpPlot(rf)
#RF???測
predicted_RF <- predict(rf, newdata = test, type = "class")
Real2 = test$Chance.of.Admit
Predict2 = predicted_RF
RF_matrix <- table(Real2, Predict2)
sum(diag(RF_matrix))/sum(RF_matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(RF_matrix), mapping = aes(x = Real2, y = Predict2)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#logit
library(arm)
logit <- bayesglm(Chance.of.Admit ~ ., data = train, family = binomial(link='logit'))
step(logit)
logit <- bayesglm(Chance.of.Admit ~ TOEFL.Score + SOP + CGPA, data = train, family = binomial(link='logit'))
predicted_LG <- predict(logit, newdata = test, type = "response")
library(InformationValue)
optcutoff <- optimalCutoff(test$Chance.of.Admit, predicted_LG)
optcutoff
predicted_LG <- ifelse(predicted_LG > optcutoff, 1, 0)
Real3 = test$Chance.of.Admit
Predict3 = predicted_LG
LG_matrix <- table(Real3, Predict3)
sum(diag(LG_matrix))/sum(LG_matrix)
plotROC(test$Chance.of.Admit, Predict3)
sensitivity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
specificity(test$Chance.of.Admit, Predict3, threshold = optcutoff)
#視覺???混淆矩???
ggplot(data =  as.data.frame(LG_matrix), mapping = aes(x = Real3, y = Predict3)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#KNN
library(class)
range <- 1:round(0.2 * nrow(train))
accuracies <- rep(NA, length(range))
for (i in range) {
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = i)
KNN_matrix <- table(test$Chance.of.Admit, predicted_KNN)
accuracies[i] <- sum(diag(KNN_matrix))/sum(KNN_matrix)
}
predicted_KNN <- knn(train = train, test = test, cl = train$Chance.of.Admit, k = which.max(accuracies))
plot(range, accuracies, xlab = "k")
which.max(accuracies) #k值
Real4 = test$Chance.of.Admit
Predict4 = predicted_KNN
KNN_matrix <- table(Real4, Predict4)
sum(diag(KNN_matrix))/sum(KNN_matrix) #???測???確???= 0.92
#視覺???混淆矩???
ggplot(data =  as.data.frame(KNN_matrix), mapping = aes(x = Real4, y = Predict4)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.92") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
# SVM
library(e1071)
svm <- svm(Chance.of.Admit ~ ., data = train, probability = TRUE)
predicted_SVM <- predict(svm, newdata = test, probability = TRUE)
Real5 = test$Chance.of.Admit
Predict5 = predicted_SVM
SVM_matrix <- table(Real5, Predict5)
sum(diag(SVM_matrix))/sum(SVM_matrix) #???測???確???= 0.98
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_matrix), mapping = aes(x = Real5, y = Predict5)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.98") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
#Tuned SVM
svm_tune <- tune(svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
#Tuned SVM
svm_tune <- tune(e1071::svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
svm_tune$best.model
plot(svm_tune)
after_tune <- svm(Chance.of.Admit ~ ., data=train, kernel="radial", cost=1, gamma=0.5)
summary(after_tune)
pred <- predict(after_tune,test)
Real6 = test$Chance.of.Admit
Predict6 = pred
SVM_Tuned_Matrix <- table(Real6, Predict6)
sum(diag(SVM_Tuned_Matrix))/sum(SVM_Tuned_Matrix) #???測???確???= 0.99
#視覺???混淆矩???
ggplot(data =  as.data.frame(SVM_Tuned_Matrix), mapping = aes(x = Real6, y = Predict6)) +
geom_tile(aes(fill = Freq), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
scale_fill_gradient(low = "white", high = "steelblue") +
ggtitle("Accuracy Score = 0.99") +
theme(plot.title = element_text(hjust = 0.5, size = 20))
#Tuned SVM
svm_tune <- tune(e1071::svm, Chance.of.Admit ~ . ,data=train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
svm_tune$best.model
plot(svm_tune)
